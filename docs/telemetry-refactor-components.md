# Telemetry Refactor Component Breakdown

| Component | Main Responsibility | Interfaces | Potential Challenges | Interactions |
| --- | --- | --- | --- | --- |
| TelemetryLatestBatch Handler (`src/routes/telemetry.ts#handleTelemetryLatestBatch`) | Serve the refactored `/api/telemetry/latest-batch` response, orchestrating scope resolution, data fetch, and presentational shaping. | Receives JSON payload from SPA; depends on `withAccess` guard, `resolveLatestBatchDevices`, `fetchLatestTelemetryBatch`, and `presentLatestBatchRow`. Emits JSON response matching new contract. | Maintaining backward compatibility with legacy payloads while feature flag is `compare`; ensuring shadow comparison noise stays actionable; guarding against unbounded device token lists. | Calls TelemetryAccess Resolver and Telemetry Store; logs via `loggerForRequest`; compares against `legacyHandleTelemetryLatestBatch` under `compare` mode. |
| TelemetrySeries Handler (`src/routes/telemetry.ts#handleTelemetrySeries`) | Produce bucketed telemetry aggregates for charts with RBAC-aware scoping. | Reads query params; consumes `resolveTelemetrySeriesConfig`, `fetchTelemetrySeries`, and `maskTelemetryNumber`; writes JSON series response. | Performance of large time windows; managing carry-forward semantics; aligning RBAC filters with SQL where clause; avoiding precision leaks in contractor scope. | Shares Telemetry Access Resolver logic; depends on Telemetry Store for D1 access; posts metrics via `loggerForRequest`; optionally shadows legacy handler. |
| Telemetry Access Resolver (`src/lib/telemetry-access.ts`) | Translate user scope + request tokens into device IDs, filters, and bindings enforced across telemetry endpoints. | Input: `Env`, `User`, request DTOs. Output: scope descriptors, SQL fragments, sanitized device lists. | Tenant-scoped masking, especially when multiple profiles overlap; defending against token fishing; staying in sync with RBAC changes; balancing caching vs. fresh lookups. | Used by both telemetry handlers; taps `buildDeviceScope`, `presentDeviceId`, and `resolveDeviceId`; returns metadata consumed by presenter utilities. |
| Telemetry Store (`src/lib/telemetry-store.ts`) | Execute D1 queries for latest batches and time-series aggregates with deterministic ordering and carry-forward logic. | Exposes `fetchLatestTelemetryBatch` and `fetchTelemetrySeries`; uses D1 `Env.DB`; relies on statement bindings from resolver. | Query optimization for large fleets; protecting against D1 hot partitions; ensuring carry-forward respects device-specific cadence; handling null metrics gracefully. | Called by handlers via Telemetry Access Resolver outputs; publishes masks to Telemetry Presentation utilities; interacts with Ops metrics for diagnostics. |
| Telemetry Feature Modes (`src/routes/telemetry.ts#getTelemetryFeatureMode`) | Toggle between `legacy`, `refactor`, and `compare` execution paths to derisk rollout. | Reads `env.TELEMETRY_REFACTOR_MODE`; clones requests for legacy comparison; writes structured logs on mismatches. | Ensuring both codepaths stay aligned; avoiding double work in production; preventing compare-mode from overwhelming logs; cleaning up after rollout. | Wraps around both handlers; calls `legacyHandleTelemetryLatestBatch/Series`; uses `loggerForRequest` and `scheduleShadowComparison`. |
| Observability Guardrails (`loggerForRequest`, `withAccess`, `recordOpsMetric`) | Provide consistent auth, logging, and metrics context for telemetry routes. | `withAccess` ensures `requireAccessUser`; logging adds route metadata; metrics record success/failure counts into D1. | Avoiding double authentication (now solved with request-level caching); capturing enough context without leaking PII; ensuring ops metrics don't overwhelm D1. | Wraps telemetry routes in `src/router/telemetry.ts`; interacts with Ops metrics cron and alerting dashboards. |

**Interactions**
- withAccess guard → Telemetry Latest Batch / Series Handlers → Telemetry Access Resolver → Telemetry Store → Response Presenter → Observability logging/metrics.
- Telemetry Feature Modes orchestrate legacy comparison in parallel, feeding results back into logging for mismatch triage.
- Production deployments keep `TELEMETRY_REFACTOR_MODE=compare` until the parity review in `docs/deployment-runbook.md` signs off, so the refactor continues shadowing legacy payloads.

Assumptions • Current D1 schema remains authoritative for telemetry data • SPA consumes JSON contracts documented in `docs/telemetry-api-design.md` • Cloudflare Worker execution time stays within current limits  
Open Questions • Should we introduce KV caching for device token lookups? • What threshold triggers turning off compare mode logs? • Do contractors need trimmed metric sets beyond current masking?  
Risks • Shadow comparisons could flood logs if schema diverges • Long series windows may exhaust D1 read limits • Missing caching might cause latency spikes under fleet growth  
Next 3 Actions • Finalize carry-forward heuristics per profile cadence • Prototype KV/Durable Object cache for token→device lookups • Schedule load test of series endpoint at projected fleet size

